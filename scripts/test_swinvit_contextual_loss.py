import autorootcwd
import torch
import torch.nn as nn
import gc
import subprocess
import re
from src.feature_extractor.swinvit_feature_extractor import SwinViTFeatureExtractor
from src.contextual_loss import SwinViTContextualLoss


def get_gpu_memory_nvidia_smi():
    """nvidia-smi ëª…ë ¹ì–´ë¡œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ MB ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            # ì¶œë ¥ ì˜ˆì‹œ: "1234, 8192" (used, total)
            match = re.search(r'(\d+),\s*(\d+)', result.stdout.strip())
            if match:
                used_mb = int(match.group(1))
                total_mb = int(match.group(2))
                return used_mb, total_mb
    except Exception as e:
        print(f"nvidia-smi ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    return 0, 0


def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def test_swinvit_contextual_loss():
    """SwinViT contextual loss í…ŒìŠ¤íŠ¸"""
    print("=== SwinViT Contextual Loss í…ŒìŠ¤íŠ¸ ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ìƒíƒœ
    used_mb, total_mb = get_gpu_memory_nvidia_smi()
    print(f"ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {used_mb} MB / {total_mb} MB")
    
    # SwinViT feature extractor ì´ˆê¸°í™”
    feature_extractor = SwinViTFeatureExtractor(
        pretrained_weight_path='weights/model_swinvit.pt'
    ).to(device)
    
    # í…ŒìŠ¤íŠ¸ìš© 3D ì´ë¯¸ì§€ ìƒì„±
    x = torch.randn(1, 1, 128, 128, 128).to(device)
    y = torch.randn(1, 1, 128, 128, 128).to(device)
    
    # Level 1 í…ŒìŠ¤íŠ¸
    level = 1
    print(f"\n--- Level {level} í…ŒìŠ¤íŠ¸ ---")
    
    # SwinViT contextual loss ì´ˆê¸°í™”
    contextual_loss = SwinViTContextualLoss(
        feature_extractor=feature_extractor,
        level=level,
        band_width=0.5,
        num_samples=5,
        neighborhood_size=8
    ).to(device)
    
    try:
        # Forward pass
        loss = contextual_loss(x, y)
        print(f"Level {level} contextual loss: {loss.item():.6f}")
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        used_mb, total_mb = get_gpu_memory_nvidia_smi()
        print(f"Forward pass í›„ GPU ë©”ëª¨ë¦¬: {used_mb} MB / {total_mb} MB")
        
        # Backward pass í…ŒìŠ¤íŠ¸
        x.requires_grad_(True)
        y.requires_grad_(True)
        loss = contextual_loss(x, y)
        loss.backward()
        
        print(f"x.grad is not None: {x.grad is not None}")
        print(f"y.grad is not None: {y.grad is not None}")
        if x.grad is not None:
            print(f"x.grad norm: {x.grad.norm().item():.6f}")
        if y.grad is not None:
            print(f"y.grad norm: {y.grad.norm().item():.6f}")
        
        # Backward pass í›„ GPU ë©”ëª¨ë¦¬ í™•ì¸
        used_mb, total_mb = get_gpu_memory_nvidia_smi()
        print(f"Backward pass í›„ GPU ë©”ëª¨ë¦¬: {used_mb} MB / {total_mb} MB")
        
    except Exception as e:
        print(f"Level {level} ì˜¤ë¥˜: {e}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del contextual_loss, feature_extractor, x, y
    clear_gpu_memory()
    
    # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
    used_mb, total_mb = get_gpu_memory_nvidia_smi()
    print(f"\nìµœì¢… GPU ë©”ëª¨ë¦¬: {used_mb} MB / {total_mb} MB")
    
    print("\nâœ… SwinViT contextual loss í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    print("ğŸš€ SwinViT Contextual Loss í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        test_swinvit_contextual_loss()
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        clear_gpu_memory()
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!") 